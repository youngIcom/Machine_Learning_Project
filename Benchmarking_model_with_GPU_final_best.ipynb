{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyXAlwmSaDkzKdNYzJbEg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngIcom/Machine_Learning_Project/blob/main/Benchmarking_model_with_GPU_final_best.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalasi library yang dibutuhkan"
      ],
      "metadata": {
        "id": "t_InPQV2dnYL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvi2PKW-dL3C"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch accelerate bitsandbytes pandas google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model Merak dan GPT 2"
      ],
      "metadata": {
        "id": "TgvOjvjadt3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, LlamaTokenizer\n",
        "import time\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "# --- Konfigurasi Gemini ---\n",
        "# GANTILAH DENGAN KUNCI API GEMINI ANDA\n",
        "# Anda bisa mendapatkan API Key di: https://ai.google.dev/\n",
        "GEMINI_API_KEY = \"AIzaSyBErar_wPD_oLuRMcVDMLqbEyW3LVu_jxI\"\n",
        "\n",
        "if GEMINI_API_KEY == \"AIzaSyBErar_wPD_oLuRMcVDMLqbEyW3LVu_jxI\":\n",
        "    print(\"\\nPERINGATAN: API Key Gemini belum diatur. Penilaian relevansi akan dinonaktifkan.\")\n",
        "    gemini_model = None\n",
        "else:\n",
        "    try:\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "        gemini_model = genai.GenerativeModel(\n",
        "            \"gemini-pro\",\n",
        "            safety_settings={\n",
        "                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "            }\n",
        "        )\n",
        "        print(\"Gemini model berhasil diinisialisasi.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal menginisialisasi Gemini model: {e}. Penilaian relevansi akan dinonaktifkan.\")\n",
        "        gemini_model = None\n",
        "\n",
        "\n",
        "# --- Konfigurasi Kuantisasi 4-bit ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Efisien untuk GPU T4\n",
        "    bnb_4bit_use_double_quant=True, # Meningkatkan akurasi\n",
        ")\n",
        "\n",
        "print(\"\\nMemuat model Merak...\")\n",
        "# --- Memuat Model Merak ---\n",
        "model_id_merak = \"Ichsan2895/Merak-7B-v4\"\n",
        "try:\n",
        "    merak_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id_merak,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    merak_tokenizer = LlamaTokenizer.from_pretrained(model_id_merak)\n",
        "    print(\"Model Merak berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat model Merak: {e}\")\n",
        "    merak_model = None\n",
        "    merak_tokenizer = None\n",
        "\n",
        "\n",
        "print(\"\\nMemuat model GPT-2...\")\n",
        "# --- Memuat Model GPT-2 ---\n",
        "gpt2_model_name = \"cahya/gpt2-small-indonesian-522M\"\n",
        "try:\n",
        "    gpt2_model = AutoModelForCausalLM.from_pretrained(\n",
        "        gpt2_model_name,\n",
        "        quantization_config=bnb_config, # Menerapkan kuantisasi juga pada GPT-2\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
        "    gpt2_pipe = pipeline(\"text-generation\", model=gpt2_model, tokenizer=gpt2_tokenizer) # Tentukan device\n",
        "    print(\"Model GPT-2 berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat model GPT-2: {e}\")\n",
        "    gpt2_model = None\n",
        "    gpt2_tokenizer = None\n",
        "    gpt2_pipe = None\n",
        "\n",
        "print(\"\\nInisialisasi model selesai.\")"
      ],
      "metadata": {
        "id": "AmdV1uixdZ0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generasi Model dengan Prompt pertanyaan"
      ],
      "metadata": {
        "id": "ssGDkg1Bd0ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fungsi Generasi untuk Merak (sesuai panduan penggunaan aslinya) ---\n",
        "def generate_merak_response(question: str, model, tokenizer, max_new_tokens: int = 256, temperature: float = 0.67, top_p: float = 0.9, top_k: int = 90) -> str:\n",
        "    if model is None or tokenizer is None:\n",
        "        return \"Model Merak tidak dimuat.\"\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"system\", \"content\": \"Anda adalah Merak, sebuah model kecerdasan buatan yang dilatih oleh Muhammad Ichsan. Mohon jawab pertanyaan berikut dengan benar, faktual, dan ramah.\"},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            do_sample=True, # Penting untuk temperature/top_p/top_k\n",
        "        )\n",
        "        response = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "\n",
        "        # Logika pemotongan respons untuk Merak\n",
        "        # Mencari marker yang menandai dimulainya respons asisten\n",
        "        assistant_start_marker = f'{question} \\n assistant\\n '\n",
        "        response_start_index = response.find(assistant_start_marker)\n",
        "        if response_start_index != -1:\n",
        "            return response[response_start_index + len(assistant_start_marker) :].strip()\n",
        "        else:\n",
        "            # Fallback jika marker tidak ditemukan, coba potong jika prompt ada di awal\n",
        "            if response.startswith(prompt):\n",
        "                 return response[len(prompt):].strip()\n",
        "            return response.strip() # Fallback terakhir\n",
        "\n",
        "\n",
        "# --- Daftar Prompt untuk Benchmark ---\n",
        "prompts = [\n",
        "    \"Siapa penulis naskah proklamasi kemerdekaan Indonesia?\",\n",
        "    \"apa yang menyebabkan perang dunia terjadi\",\n",
        "    \"budi membeli sebuah roti dengan harga 10000, dan kemudian dia membayar dengan uang 20000 berapa sisa kembalian uang budi\",\n",
        "    \"apa yang dimaksud dengan metode\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "# --- Parameter Generasi untuk Merak ---\n",
        "merak_gen_params = dict(\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.67,\n",
        "    top_p=0.9,\n",
        "    top_k=90,\n",
        ")\n",
        "\n",
        "# --- Parameter Generasi untuk GPT-2 ---\n",
        "gpt2_gen_params = {\n",
        "    \"max_length\": 100, # Batas total token output (termasuk prompt)\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"num_beams\": 2, # Menggunakan beam search\n",
        "    \"do_sample\": False, # Nonaktifkan sampling jika num_beams > 1\n",
        "    \"no_repeat_ngram_size\": 2,\n",
        "    \"eos_token_id\": gpt2_tokenizer.eos_token_id if gpt2_tokenizer else None # Optional\n",
        "}\n",
        "\n",
        "print(\"\\nMemulai proses benchmark...\")\n",
        "for prompt in prompts:\n",
        "    # === MERAK BENCHMARK ===\n",
        "    if merak_model and merak_tokenizer:\n",
        "        print(f\"\\n[{prompt}] Menjalankan Merak...\")\n",
        "        start_time = time.time()\n",
        "        merak_response = generate_merak_response(prompt, merak_model, merak_tokenizer, **merak_gen_params)\n",
        "        merak_time = time.time() - start_time\n",
        "\n",
        "        relevance_score_merak = \"N/A (Gemini tidak aktif)\"\n",
        "        if gemini_model:\n",
        "            gemini_prompt_merak = (\n",
        "                f\"Nilai relevansi respons berikut terhadap prompt asli dalam skala 1 hingga 5 \"\n",
        "                f\"(1 = tidak relevan, 5 = sangat relevan).\\n\"\n",
        "                f\"Prompt Asli: {prompt}\\nRespons Model: {merak_response}\\nNilai Relevansi (1-5):\"\n",
        "            )\n",
        "            try:\n",
        "                gemini_response_merak = gemini_model.generate_content(gemini_prompt_merak)\n",
        "                score_str = ''.join(filter(str.isdigit, gemini_response_merak.text.strip()))\n",
        "                relevance_score_merak = int(score_str[0]) if score_str else \"Error\"\n",
        "            except Exception as e:\n",
        "                print(f\"[Merak] Error saat menilai relevansi dengan Gemini: {e}\")\n",
        "                relevance_score_merak = \"Error\"\n",
        "    else:\n",
        "        merak_time = \"N/A\"\n",
        "        merak_response = \"Model Merak gagal dimuat.\"\n",
        "        relevance_score_merak = \"N/A\"\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": \"Merak-7B-v4\",\n",
        "        \"Prompt\": prompt,\n",
        "        \"Latency (s)\": f\"{merak_time:.4f}\" if isinstance(merak_time, float) else merak_time,\n",
        "        \"Response\": merak_response,\n",
        "        \"Relevance Score (1-5)\": relevance_score_merak\n",
        "    })\n",
        "\n",
        "    # === GPT-2 BENCHMARK ===\n",
        "    if gpt2_pipe:\n",
        "        print(f\"[{prompt}] Menjalankan GPT-2...\")\n",
        "        start_time = time.time()\n",
        "        gpt2_output = gpt2_pipe(prompt, **gpt2_gen_params)\n",
        "        gpt2_response = gpt2_output[0]['generated_text'].strip()\n",
        "        gpt2_time = time.time() - start_time\n",
        "\n",
        "        # Potong prompt dari respons jika pipeline mengembalikannya\n",
        "        if gpt2_response.startswith(prompt):\n",
        "            gpt2_response = gpt2_response[len(prompt):].strip()\n",
        "\n",
        "        relevance_score_gpt2 = \"N/A (Gemini tidak aktif)\"\n",
        "        if gemini_model:\n",
        "            gemini_prompt_gpt2 = (\n",
        "                f\"Nilai relevansi respons berikut terhadap prompt asli dalam skala 1 hingga 5 \"\n",
        "                f\"(1 = tidak relevan, 5 = sangat relevan).\\n\"\n",
        "                f\"Prompt Asli: {prompt}\\nRespons Model: {gpt2_response}\\nNilai Relevansi (1-5):\"\n",
        "            )\n",
        "            try:\n",
        "                gemini_response_gpt2 = gemini_model.generate_content(gemini_prompt_gpt2)\n",
        "                score_str = ''.join(filter(str.isdigit, gemini_response_gpt2.text.strip()))\n",
        "                relevance_score_gpt2 = int(score_str[0]) if score_str else \"Error\"\n",
        "            except Exception as e:\n",
        "                print(f\"[GPT2] Error saat menilai relevansi dengan Gemini: {e}\")\n",
        "                relevance_score_gpt2 = \"Error\"\n",
        "    else:\n",
        "        gpt2_time = \"N/A\"\n",
        "        gpt2_response = \"Model GPT-2 gagal dimuat.\"\n",
        "        relevance_score_gpt2 = \"N/A\"\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": \"gpt2-small-indonesian-522M\",\n",
        "        \"Prompt\": prompt,\n",
        "        \"Latency (s)\": f\"{gpt2_time:.4f}\" if isinstance(gpt2_time, float) else gpt2_time,\n",
        "        \"Response\": gpt2_response,\n",
        "        \"Relevance Score (1-5)\": relevance_score_gpt2\n",
        "    })\n",
        "\n",
        "print(\"\\nBenchmark selesai.\")"
      ],
      "metadata": {
        "id": "_NE7h83Bdeqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tampilkan output pada tabel"
      ],
      "metadata": {
        "id": "90hqRi4fd7Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Menampilkan Hasil dalam Format Tabel ---\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             HASIL BENCHMARK MODEL             \")\n",
        "print(\"=\"*50)\n",
        "print(df_results.to_markdown(index=False))\n",
        "print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "zOb8GXGbdiMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tampilkan respon pada tabel"
      ],
      "metadata": {
        "id": "xMQewi7ud-30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)  # supaya respons panjang tidak terpotong\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# (Opsional) urutkan kolom agar rapi\n",
        "ordered_cols = [\"Model\",\n",
        "                \"Prompt\",\n",
        "                \"Latency (s)\",\n",
        "                \"Relevance Score (1-5)\",\n",
        "                \"Response\"]\n",
        "results_df = results_df[ordered_cols]\n",
        "\n",
        "# Ganti titik desimal dengan koma agar konsisten dengan format sebelumnya\n",
        "results_df[\"Latency (s)\"] = results_df[\"Latency (s)\"].astype(str).str.replace('.', ',', regex=False)\n",
        "\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "d6en16yTdj-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tampilkan pada spreadsheet"
      ],
      "metadata": {
        "id": "xvNLjeayeHcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=results_df)"
      ],
      "metadata": {
        "id": "lkjG5CikeLCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}